{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a523400",
   "metadata": {},
   "source": [
    "<font size=\"1.5\">Copyright 2021, by the California Institute of Technology. ALL RIGHTS RESERVED. United States Government sponsorship acknowledged. Any commercial use must be negotiated with the Office of Technology Transfer at the California Institute of Technology.\n",
    "\n",
    "<font size=\"1.5\">This software may be subject to U.S. export control laws and regulations. By accepting this document, the user agrees to comply with all applicable U.S. export laws and regulations. User has the responsibility to obtain export licenses, or other export authority as may be required, before exporting such information to foreign countries or providing access to foreign persons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7927b8",
   "metadata": {},
   "source": [
    "# UAVSAR SLC stack processor\n",
    "\n",
    "<font size=\"3\">The notebook uses functions from ISCE StripmapStack and some additional functionalities in uavsar_utils folder to create Interferograms from SLC data. \n",
    "\n",
    "<font size=\"3\">Currently there is no support to automatically fetch the data. The users are required to copy the download urls from the [UAVSAR webpage](https://uavsar.jpl.nasa.gov/) into a text file locally. \n",
    "- If multiple segments exists, download urls for all the segments are needed.\n",
    "- The notebook reuires ISCE environment. (The last three cells require ISCE + MintPy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d7906b-5d95-4e47-90c9-5b175fd2c15f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import glob\n",
    "import shelve\n",
    "import isce\n",
    "import isceobj\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "n_processes = 4 #int(multiprocessing.cpu_count()) - 2;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ffa52",
   "metadata": {},
   "source": [
    "### Add the required paths\n",
    "<font size=\"3\">Add path to the ISCE stripmapStack directory and import functions from the uavsar_utils directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd0ddd-2d0b-443e-b1ac-93205d37881c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get the path of the ISCE base dir\n",
    "env_str = os.popen(\"conda env list | grep '/isce$' | awk '{print $NF}'\")\n",
    "isce_base_dir = env_str.read().strip();\n",
    "isce2_share_dir = f\"{isce_base_dir}/share/isce2\";\n",
    "\n",
    "# now add stripmapStack contrib directory to PATH \n",
    "stripmapStack_dir = f\"{isce2_share_dir}/stripmapStack\";\n",
    "# add UAVSAR functionalities to the path\n",
    "uavsar_utils_dir = '/home/jovyan/atbd-se-development/uavsar_utils'\n",
    "os.environ['PATH'] = f\"{uavsar_utils_dir}:{stripmapStack_dir}:{os.environ['PATH']}\"\n",
    "if uavsar_utils_dir not in sys.path:\n",
    "    sys.path.append(uavsar_utils_dir)\n",
    "!export OMP_NUM_THREADS=16\n",
    "try:\n",
    "    from insar import ImageReader, Interferogram, correlation, cpxlooks, rilooks, isce_xml_ifg\n",
    "    from unwrap_wlc import unwrap,unwrap_post,unwrap_pre,apply_phasemask,isce_xml_unw\n",
    "except:\n",
    "    print('Error: Unble to import key functions. Add uavsar_utils directory to pythonpath')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d03da4",
   "metadata": {},
   "source": [
    "### Run function to execute commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49e135-39a8-48a0-98a3-55f77f932c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_cmd(cmd):\n",
    "    print(\"Command: {}\".format(cmd))\n",
    "    try:\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Command failed with error: {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a2ebde-05ea-416c-80aa-99c81d1da6e7",
   "metadata": {},
   "source": [
    "## Place all your inputs here. \n",
    "1. Point to your scratch directory or where you would process the data\n",
    "2. Give a folder name for the processing. \n",
    "3. Specify number of looks and number of connections per SLC.\n",
    "4. Choose a reference data if not the first data will be chosen as reference data\n",
    "5. A bounding box is required to download the DEM. If not given will be calculated later from the metadata. If there exists an independent DEM file in ISCE readable format, point the path as 'dem_file\n",
    "6. Custom unwrapping with snaphu configuration parameters is implemented. Requires local installation of snaphu. If not choose 'isce' as unwrapping protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9462ccd2-9968-4846-9e50-b5e28f56845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### In put your Scratch dir\n",
    "scratch_dir = '/scratch/uavsar'; #define your scratch dir\n",
    "\n",
    "track_name = 'track_23013'  # Define the UAVSAR directory name\n",
    "segments = [1, 2]  # Define the SLC segments; Used for concatenation\n",
    "#reference_date = None # input a reference date if not the first date will be considered reference date\n",
    "\n",
    "###### Looks and Connections Options #########\n",
    "rlks = 3  # Define range looks\n",
    "alks = 12  # Define azimuth looks\n",
    "numConnections = 2  # Define number of interferogram pairs to be made with each SLC similar to ISCE\n",
    "\n",
    "print('Processing options\\n'\n",
    "      'range looks={} \\n'\n",
    "      'azimuth looks = {} \\n'\n",
    "      'connections per slc:{} \\n'.format(rlks,alks,numConnections));\n",
    "\n",
    "##### Optional input parameters ########\n",
    "# reference_date = '20240101'\n",
    "# bbox = '35 37 -122 -120' # Bounding box for DEM download, if not given will be calculated from the ann file\n",
    "\n",
    "####### Unwrapping options #############\n",
    "unwrapping_protocol = 'custom'  # ['isce' or 'custom']\n",
    "if unwrapping_protocol == 'custom':\n",
    "    print('Unwrapping will be done using {} options\\n'.format(unwrapping_protocol))\n",
    "    additional_rlks = 3  # default = 3\n",
    "    additional_alks = 3  # default = 3 \n",
    "    downsample_interferograms = True  # default= None; If 'True' unwrapping will be done on multilooked interferograms at a lower resolution and the phase cycles obtained will be applied to the \n",
    "\n",
    "    snaphu_conf = {\n",
    "        'DEFAULT_COR_REJECT': 0.4,  # default = 0.4 # Pixels with coherence threshold below will be discarded for unwrapping\n",
    "        'DEFAULT_COR_SOURCE': 0.45,  # default = 0.45 # phase for pixels from discarded above will be interpolated using nearest pixels with coherence above this\n",
    "        'DEFAULT_RADIUS': 3.0  # default = 3.0\n",
    "    }\n",
    "### custom installatation needs snaphu installed locally\n",
    "if unwrapping_protocol == 'custom':\n",
    "    snaphu_path= os.getenv(\"HOME\")+'/'+'tools/bin';\n",
    "    if os.path.exists(snaphu_path):\n",
    "        os.environ['PATH'] = f\"{snaphu_path}:{os.environ['PATH']}\";\n",
    "    snaphu_test = shutil.which('snaphu');\n",
    "    if snaphu_test:\n",
    "        print('Snaphu Installation is works. \\n');\n",
    "        print(snaphu_conf,'\\n')\n",
    "    else:\n",
    "        print('Locally installed SNAPHU is necessary for implementing custom unwrapping. \\n');\n",
    "        print('Install snaphu or input snaphu bin path. \\n'); \n",
    "        print('Else choose unwrapping protocol as isce. \\n');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d3b8b",
   "metadata": {},
   "source": [
    "### Create processing directory\n",
    "- Creates a directory as pointed earlier. Rises an error if the scratch directory doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c69d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the scratch directory exists\n",
    "if not os.path.exists(scratch_dir):\n",
    "    print(f'Error: {scratch_dir} does not exist.')\n",
    "else:\n",
    "    \n",
    "    ######## Creating Main directories ###########\n",
    "    out_dir = os.path.join(scratch_dir, track_name)\n",
    "    os.makedirs(out_dir, exist_ok=True);\n",
    "    os.chdir(out_dir)\n",
    "    print(f'Outputs will be saved to {out_dir}')\n",
    "\n",
    "    download_dir = os.path.join(out_dir, 'download')\n",
    "    os.makedirs(download_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae609e3",
   "metadata": {},
   "source": [
    "### Write the download links\n",
    "- Create a file named download_links.sh in the download folder and copy all your links in it. UAVSAR webpage has 'wget' populated infront of the links already.\n",
    "- If download is not required, copy the UAVSAR SLCs and *.ann files into the download directory.\n",
    "- If rerunning is required, move the existing *.slc files from the download directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d01f4e-a1e8-42ca-88eb-0ea8da4a911a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#copy the links from UAVSAR webpage into a file in the download dir named download_links.sh\n",
    "#this cell shall be updated if a better method of downloading UAVSAR data is available\n",
    "if any(os.path.exists(os.path.join(out_dir, 'SLC')) or os.path.exists(os.path.join(out_dir, 'SLC_seg'+str(f))) for f in segments):\n",
    "    print('SLC folder exist. Skipping download');\n",
    "else:\n",
    "    print('Downloading UAVSAR SLCs using wget links');\n",
    "    downloaded_slcs= len(glob.glob(os.path.join(download_dir, '*.slc')));\n",
    "    if downloaded_slcs<1:\n",
    "        os.chdir(download_dir);\n",
    "        !sh download_links.sh > download_log\n",
    "        !rm *.ann.*\n",
    "        !rm *.dop.*\n",
    "        os.chdir(out_dir);\n",
    "    else:\n",
    "        print('SLCs exist. Continue to create SLC dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699307cd",
   "metadata": {},
   "source": [
    "### Create SLC directory for  one/multiple SLC segments\n",
    "- A directory for each segment (Ex:SLC_seg1) is created. If only one segment then SLC directory will be created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fecfc3-ca56-4a2f-bac9-903597a8128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the dop file to the out dir\n",
    "dop_file = glob.glob(os.path.join(download_dir, '*.dop'))[0];\n",
    "if not os.path.exists(dop_file):\n",
    "    print('Error: *.dop file does not exitst');\n",
    "#Create SLC directory\n",
    "if len(segments)>1:\n",
    "    print('Multiple Segments exist. Creating multiple SLC segments directories!')\n",
    "    for i in range(len(segments)):\n",
    "        slc_seg_dir = 'SLC_seg{}'.format(segments[i]);\n",
    "        if not os.path.exists(slc_seg_dir):\n",
    "            cmd = 'prepareUAVSAR_coregStack.py -i {} -d {} -o {} -s {}'.format(download_dir,dop_file,slc_seg_dir,segments[i]);\n",
    "            print(cmd);\n",
    "            os.system(cmd);\n",
    "            if i< len(segments)-1:\n",
    "                cmd = 'cp {}/*/*.ann {}'.format(slc_seg_dir,download_dir);\n",
    "                print(cmd);\n",
    "                os.system(cmd);\n",
    "    slc_dir = os.path.join(out_dir,'SLC_seg{}'.format(segments[0])); \n",
    "else:\n",
    "    print('Procesing only One Segment. No Concatenation')\n",
    "    slc_dir = os.path.join(out_dir,'SLC');\n",
    "    cmd = 'prepareUAVSAR_coregStack.py -i {} -d {} -o {} -s {}'.format(download_dir,dop_file,slc_dir,segments[0]);\n",
    "    os.system(cmd);\n",
    "\n",
    "print('SLC directory:',slc_dir);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5562bac",
   "metadata": {},
   "source": [
    "### Give a reference data if not the first data will be considered reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c303b43-7525-4696-af35-74522de403a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dateList = sorted(os.listdir(slc_dir));\n",
    "print('SLC dates:',dateList);\n",
    "try:\n",
    "    reference_date\n",
    "except:\n",
    "    reference_date=sorted(os.listdir(slc_dir))[0];\n",
    "print('Reference date for Meta data:',reference_date);\n",
    "reference_slc_dir = os.path.join(slc_dir,reference_date);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754ac39",
   "metadata": {},
   "source": [
    "### Read Metadata from the reference SLC\n",
    "\n",
    "- Here SLC meatadata is loaded into a dictionary variable to query for samples (columns), rows and bounding box for DEM.Each annotation file contains the info for all the segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ff7a5-9a27-4ae7-961f-4dcbfb75a790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read necessary info from the UAVSAR annotation file. Each annotation file contains the info for all the segments.\n",
    "def read_uavsar_ann_file(ann_file_path):\n",
    "    '''read UAVSAR ann file as a dict to query for rows, columns info '''\n",
    "    result_dict = {}\n",
    "    with open(ann_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Strip any leading/trailing whitespace and skip empty lines\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(';'):\n",
    "                continue\n",
    "            \n",
    "            # Split the line into key and value using '=' as the separator\n",
    "            if '=' in line:\n",
    "                key, value = line.split('=', 1);\n",
    "                key = key.split('(')[0].strip()\n",
    "                value = value.split(';')[0].strip()\n",
    "                result_dict[key.strip()] = value.strip()\n",
    "    \n",
    "    return result_dict\n",
    "ann_file_path = glob.glob(os.path.join(reference_slc_dir, '*.ann'))[0];\n",
    "slc_info_dict = read_uavsar_ann_file(ann_file_path);\n",
    "key = 'slc_{}_1x1 Columns'.format(str(segments[0]))\n",
    "samples = int(slc_info_dict[key]);\n",
    "lines = 0;\n",
    "for seg in segments:\n",
    "    key = 'slc_{}_1x1 Rows'.format(str(seg))\n",
    "    lines += int(slc_info_dict[key]);\n",
    "print('total samples/columns:',samples);\n",
    "print('total rows:',lines) \n",
    "lats= [];lons= [];\n",
    "try:\n",
    "    bbox\n",
    "except:\n",
    "    for seg in segments:\n",
    "        for corner in [1,2,3,4]:\n",
    "            lat, lon = slc_info_dict['Segment {} Data Approximate Corner 1'.format(seg,corner)].split(',')\n",
    "            lats.append(float(lat));lons.append(float(lon));\n",
    "    bbox = ' '.join(map(str,[int(np.floor(np.min(lats)-0.1)),int(np.ceil(np.max(lats)+0.1)),int(np.floor(np.min(lons)-0.1)),int(np.ceil(np.max(lons)+0.1))]));\n",
    "print('Bounding box for DEM:',bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348610c",
   "metadata": {},
   "source": [
    "### Update SLC metadata if multiple segments exist\n",
    "- Total length after concatenating all segments is updated. Coordinates of lower right corner from the last segment is updated\n",
    "- Changes are made to the metadata of the first SLC segment which will be used subsequently in the other steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(segments)>1:\n",
    "    first_seg_dir = os.path.join(out_dir,'SLC_seg{}'.format(segments[0]));\n",
    "    last_seg_dir = os.path.join(out_dir,'SLC_seg{}'.format(segments[-1]));\n",
    "    for date in dateList:\n",
    "        cmd = 'uavsar_update_shelve.py -i {} -d {} -l {} -s2 {}'.format(out_dir,os.path.join(first_seg_dir,date,'data'),lines, os.path.join(last_seg_dir,date,'data'));\n",
    "        os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132e2c3",
   "metadata": {},
   "source": [
    "### Download SRTM DEM \n",
    "\n",
    "- If using own DEM is preferred, point to it as dem_file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548cabe-c407-42e7-8399-f9dab6ebb93d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get DEM for geocoding\n",
    "#if you have your DEM file deifne its path as dem_file '/path/to/dem_file' if not SRTM DEM will be used\n",
    "#define bounding box for the DEM\n",
    "try:\n",
    "    dem_file\n",
    "except:\n",
    "    dem_dir = os.path.join(out_dir,'DEM');\n",
    "    os.makedirs(dem_dir, exist_ok=True);\n",
    "    try:\n",
    "        dem_file = glob.glob(os.path.join(dem_dir, \"*.dem.wgs84\"))[0];\n",
    "    except:\n",
    "        os.chdir(dem_dir);\n",
    "        source_link =\"http://step.esa.int/auxdata/dem/SRTMGL1/\"\n",
    "        try:\n",
    "            bbox\n",
    "        except:\n",
    "            print('Bounding box for DEM download required');\n",
    "        cmd ='dem.py -a stitch -b {} -u {} -d {} -r -s 1 -c'.format(bbox,source_link,dem_dir);\n",
    "        print(cmd);\n",
    "        run_cmd(cmd);\n",
    "        cmd = 'fixImageXml.py -f -i {}'.format(dem_file)\n",
    "        print(cmd);\n",
    "        os.system(cmd);\n",
    "        os.chdir(out_dir);\n",
    "        dem_file = glob.glob(os.path.join(dem_dir, \"*.dem.wgs84\"))[0];\n",
    "print('DEM:{}\\n'.format(dem_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe153f9",
   "metadata": {},
   "source": [
    "### Create interferogram pairs list\n",
    "- Based on the numConnections variable input earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e6de1-874d-4c6e-ba36-f50016d6d6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create pairs\n",
    "looks = [alks, rlks];\n",
    "ifg_dir =  os.path.join(out_dir,'Igrams');\n",
    "print('Selecting pairs with {} connections per SLC \\n'.format(numConnections));\n",
    "os.makedirs(ifg_dir, exist_ok=True);\n",
    "pairs_list= [];\n",
    "for ii,dd in enumerate(dateList):\n",
    "    referenceDate = dd;\n",
    "    for jj in range(ii+1, ii+1+numConnections):\n",
    "        if jj < len(dateList):\n",
    "            secondaryDate = dateList[jj];\n",
    "            ifg_pair =  referenceDate+'_'+secondaryDate;\n",
    "            pairs_list.append(ifg_pair);\n",
    "print('Pairs:','\\n')\n",
    "print(pairs_list,'\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b595268",
   "metadata": {},
   "source": [
    "### Generate Interferometic products\n",
    "- The code multilooks the SLCs and creates interferograms. Processes multiple segments simultenously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e4c14-acb4-48ba-ac04-55c8c56aa860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Genrating Interferogram, Amplitude and coherence files \\n');\n",
    "print('Interferogram is concatenated from multiple SLC segments');\n",
    "\n",
    "def cat_and_interfere(slc_dir,samples,looks,segments,ifg_dir,pair):\n",
    "    referenceDate,secondaryDate = pair.split('_');\n",
    "    int_dir  = os.path.join(ifg_dir,referenceDate+'_'+secondaryDate);os.makedirs(int_dir, exist_ok=True);\n",
    "    intf =  os.path.join(int_dir,referenceDate+'_'+secondaryDate+'.int');\n",
    "    ampf =  os.path.join(int_dir,referenceDate+'_'+secondaryDate+'.amp');\n",
    "    cohf =  os.path.join(int_dir,referenceDate+'_'+secondaryDate+'.coh');\n",
    "    igram = Interferogram(samples, looks);\n",
    "    int_samples = samples//looks[1];\n",
    "    if not all(os.path.exists(f) for f in (intf, ampf, cohf)):\n",
    "        if len(segments)>1:\n",
    "            with open(intf, 'wb') as fint, open(ampf, 'wb') as famp:\n",
    "                for seg in segments:\n",
    "                    ref_img = slc_dir[:-1]+str(seg)+'/'+referenceDate+'/'+referenceDate+'.slc';#print(ref_img)\n",
    "                    sec_img = slc_dir[:-1]+str(seg)+'/'+secondaryDate+'/'+secondaryDate+'.slc';#print(sec_img)\n",
    "                    img0 = ImageReader(ref_img, samples, dtype='complex64')\n",
    "                    img1 = ImageReader(sec_img, samples, dtype='complex64')\n",
    "                    #log.info('... %s * conj(%s)', img0.filename, img1.filename)\n",
    "                    for int_row, amp_row in igram.iterrows(img0, img1):\n",
    "                        int_row.tofile(fint);\n",
    "                        amp_row.tofile(famp);\n",
    "            correlation(intf, ampf, cohf);\n",
    "            isce_xml_ifg(intf, ampf, cohf, int_samples)\n",
    "        else:\n",
    "            ref_img = slc_dir+'/'+referenceDate+'/'+referenceDate+'.slc';#print(ref_img)\n",
    "            sec_img = slc_dir+'/'+secondaryDate+'/'+secondaryDate+'.slc';#print(sec_img)\n",
    "            with open(intf, 'wb') as fint, open(ampf, 'wb') as famp:\n",
    "                img0 = ImageReader(ref_img, n, dtype='complex64')\n",
    "                img1 = ImageReader(sec_img, n, dtype='complex64')\n",
    "                #log.info('... %s * conj(%s)', img0.filename, img1.filename)\n",
    "                for int_row, amp_row in igram.iterrows(img0, img1):\n",
    "                        int_row.tofile(fint);\n",
    "                        amp_row.tofile(famp);\n",
    "            correlation(intf, ampf, cohf);\n",
    "            isce_xml_ifg(intf, ampf, cohf, int_samples)\n",
    "    else:\n",
    "        print('Skipping {} as files exist \\n'.format(os.path.basename(intf)));\n",
    "with multiprocessing.Pool(processes=12) as pool:\n",
    "    pool.starmap(cat_and_interfere,[(slc_dir,samples,looks,segments,ifg_dir,pair) for pair in pairs_list]);\n",
    "print('Finished generating Inteferograms');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b2339",
   "metadata": {},
   "source": [
    "### Unwrap the interferograms using Custom option\n",
    "1. The process requires snaphu\n",
    "2. The process replaces phase on low coherence pixels with interpolated phase from nearest high coherence pixels\n",
    "3. Performs unwrapping initially on a low resoution interferogram and adds the corresponding number of phase cycles to the high resolution interferogram. Enhances the processing time and performance of unwrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1819ca-ec79-4b0e-8ecd-c8051c602eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#unwrap interferograms\n",
    "\n",
    "if unwrapping_protocol == 'custom':\n",
    "    import json\n",
    "    ifg_info = {\n",
    "      \"looks_azimuth\": alks,\n",
    "      \"looks_range\": rlks,\n",
    "      \"samples_int\": samples//rlks,\n",
    "      \"samples_slc\": samples\n",
    "    }\n",
    "    int_json_file = os.path.join(ifg_dir,'int.json');\n",
    "    with open(int_json_file,'w') as json_file:\n",
    "        json.dump(ifg_info,json_file,indent=4)\n",
    "    for pair in pairs_list:\n",
    "        shutil.copy(int_json_file,os.path.join(ifg_dir,pair))\n",
    "        \n",
    "    ### function to downsamsaple interferograms\n",
    "    def downsample_int(hires_dir,lores_dir,nr,na,pair):\n",
    "        dir_in = os.path.join(hires_dir, pair);\n",
    "        dir_out = os.path.join(lores_dir, pair);\n",
    "        if dir_in == dir_out:\n",
    "            raise ValueError('Input and output directories must differ \\n')\n",
    "\n",
    "        dir_out = os.path.abspath(dir_out)\n",
    "        os.makedirs(dir_out, exist_ok=True);\n",
    "        # Load the JSON file describing the interferograms and update it with the\n",
    "        # new number of looks.\n",
    "        input_json_file= os.path.join(dir_in,'int.json');\n",
    "        output_json_file= os.path.join(dir_out,'int.json');\n",
    "        if not os.path.exists(input_json_file):\n",
    "            raise ValueError('Missing int.json file in the high resolution interferogram directory \\n')\n",
    "\n",
    "        with open(input_json_file) as f:\n",
    "            info = json.load(f)\n",
    "\n",
    "        n = info['samples_int']\n",
    "        info['samples_int'] = n // nr\n",
    "        info['looks_range'] *= nr\n",
    "        info['looks_azimuth'] *= na\n",
    "\n",
    "        with open(output_json_file, 'w') as f:\n",
    "            json.dump(info, f, indent=2, sort_keys=True)\n",
    "\n",
    "        looks = (na, nr)\n",
    "\n",
    "        intf_in = glob.glob(os.path.join(dir_in,'*.int'))[0];\n",
    "        amp_in = intf_in[:-4] + '.amp';\n",
    "        if not os.path.exists(amp_in):\n",
    "            raise IOError('Found int=%s but no matching .amp file.' % intf_in)\n",
    "        intf_out = os.path.join(dir_out, os.path.basename(intf_in))\n",
    "        amp_out = os.path.join(dir_out, os.path.basename(amp_in))\n",
    "        cor_out = amp_out[:-4] + '.coh';\n",
    "        if not all(os.path.exists(f) for f in (intf_out, amp_out, cor_out)):\n",
    "            cpxlooks(intf_in, intf_out, n, looks)\n",
    "            rilooks(amp_in, amp_out, n, looks)\n",
    "            correlation(intf_out, amp_out, cor_out)\n",
    "        else:\n",
    "            print('Skipping downsampling of {}. Downsampled files already exist \\n'.format(os.path.basename(intf_in)))\n",
    "    \n",
    "    ### function to run wrapping on a downsampled interferograms and phase ambiguity solution to original interferograms\n",
    "\n",
    "    def unwrap_high_low(lores_ifg_dir,hires_ifg_dir,pair,snaphu_conf,clean=False):\n",
    "        dir_hi = os.path.join(hires_ifg_dir, pair);\n",
    "        dir_low = os.path.join(lores_ifg_dir, pair);\n",
    "        cor_reject = snaphu_conf['DEFAULT_COR_REJECT']\n",
    "        cor_source = snaphu_conf['DEFAULT_COR_SOURCE']\n",
    "        radius = snaphu_conf['DEFAULT_RADIUS']\n",
    "        if dir_hi is None:\n",
    "            dir_hi = dir_low\n",
    "\n",
    "        for intf in glob.glob(os.path.join(dir_hi, '*.int')):\n",
    "            # If --highres=dir_hi option is given, assume that the high-resolution\n",
    "            # products are the ones of interest (e.g., still skip if low-res unw\n",
    "            # does not exist).\n",
    "            unw = intf[:-4] + '.unw'\n",
    "            if os.path.exists(unw):\n",
    "                print('Skipping because {} it already exists.'.format(os.path.basename(unw)));\n",
    "            else:\n",
    "                # Make sure necessary input files exist.\n",
    "                intf_low = os.path.join(dir_low, os.path.split(intf)[1])\n",
    "                cor_low = intf_low[:-4] + '.coh' # TO modif\n",
    "                if os.path.isfile(cor_low) == False:\n",
    "                    cor_low = intf_low[:-4] + '.cor'\n",
    "                info_low = os.path.join(dir_low, 'int.json')\n",
    "                for f in (intf_low, cor_low, info_low):\n",
    "                    if not os.path.exists(f):\n",
    "                        raise IOError('Could not find %s needed to make %s.' % (f, unw))\n",
    "                # checking of the low res product already exists, if yes skip it as it could be phase corrected\n",
    "                unw_low = intf_low.replace('.int', '.unw')      # DB: start 29 June 2016\n",
    "                if os.path.exists(unw_low):\n",
    "                    log.debug('Skipping %s because it already exists.\\n', unw_low)\n",
    "                    pre_int = intf_low + '.pre'\n",
    "                    pre_info = intf_low + '.pre.h5'\n",
    "                else:\n",
    "                    # Inpaint low correlation areas of the interferogram.\n",
    "                    with open(info_low) as f:\n",
    "                        n = json.load(f)['samples_int']\n",
    "                    pre_int = intf_low + '.pre'\n",
    "                    pre_info = intf_low + '.pre.h5'\n",
    "                    unwrap_pre(intf_low, cor_low, n, pre_int, pre_info,\n",
    "                               cor_reject=snaphu_conf['DEFAULT_COR_REJECT'], cor_source=snaphu_conf['DEFAULT_COR_SOURCE'], \n",
    "                               sigma=snaphu_conf['DEFAULT_RADIUS']);\n",
    "                    # Unwrap the simplified interferogram with SNAPHU.\n",
    "                    pre_unw = pre_int + '.unw'\n",
    "        #            unwrap(pre_int, pre_unw)   # original version of the code \n",
    "                    unwrap(pre_int, pre_unw, cor_low)   # Use this command if you want snaphu to account for real coherence\n",
    "                    # Put the original data back in on a reasonable phase ambiguity.\n",
    "                    unw_low = intf_low.replace('.int', '.unw')\n",
    "                    unwrap_post(pre_unw, pre_info, unw_low)     # DB: end \n",
    "\n",
    "                # Apply the low resolution phase ambiguity solution to high res data.\n",
    "                if unw != unw_low:\n",
    "                    apply_phasemask(unw_low, intf, unw)\n",
    "                if clean==True:\n",
    "                    for f in (pre_int, pre_info):\n",
    "                        if os.path.exists(f):        #DB: start 29 June 2016\n",
    "                            os.remove(f)             #DB: en\n",
    "\n",
    "    def generate_xml_for_unw(ifg_dir,pair,slc_dir,looks):\n",
    "        unwrapFile = \"{b}/{a}/{a}.unw\".format(a=pair, b=ifg_dir);\n",
    "        int_file = \"{b}/{a}/{a}.int\".format(a=pair, b=ifg_dir)\n",
    "        unw_xml_file = unwrapFile+'.xml';\n",
    "        if not os.path.exists(unw_xml_file):\n",
    "            ## Amplitude layer, taken from the interferogram\n",
    "            ds = gdal.Open(int_file, gdal.GA_ReadOnly)\n",
    "            igram = ds.GetRasterBand(1).ReadAsArray();\n",
    "            length, width = np.shape(igram)\n",
    "            #print ('width, length =', width, length)\n",
    "            ds = None;\n",
    "            if os.path.getsize(unwrapFile) != os.path.getsize(int_file):    \n",
    "                amp = np.abs(igram) # Compute amplitude from ifg\n",
    "\n",
    "                ## Generate 2 band unw product by adding amp file to unwrapped ifgram\n",
    "                # add amp file to unwrapped ifgram\n",
    "                f1 = unwrapFile\n",
    "                X1 = np.memmap(f1, dtype=np.float32)\n",
    "                igram_unw = X1.reshape((length,width))\n",
    "                unw_pha = igram_unw\n",
    "\n",
    "                ## Generate 2 band unw product\n",
    "                new_unw = unwrapFile\n",
    "                unw_data = np.array(np.hstack((amp, unw_pha)), dtype=np.float32)\n",
    "                unw_data.tofile(new_unw);\n",
    "            \n",
    "            ## generate referenceShelve dir\n",
    "            referenceDate,secondaryDate = pair.split('_');\n",
    "            reference_slc_dir = os.path.join(slc_dir,referenceDate);\n",
    "            interferogramDir = os.path.dirname(int_file)\n",
    "            referenceShelveDir = os.path.join(interferogramDir , 'referenceShelve')\n",
    "            os.makedirs(referenceShelveDir, exist_ok=True);\n",
    "            cpCmd='cp ' + os.path.join(reference_slc_dir, 'data*') +' '+referenceShelveDir\n",
    "            os.system(cpCmd)\n",
    "            pckfile = os.path.join(referenceShelveDir,'data');\n",
    "\n",
    "            ## generate xml file for unw\n",
    "            isce_xml_unw(unwrapFile, width,length,2)\n",
    "            \n",
    "        else:\n",
    "            print('Skipping {} as xml file exists'.format(os.path.basename(unwrapFile)));\n",
    "    \n",
    "    if downsample_interferograms == True:\n",
    "        lowres_ifg_dir= os.path.join(out_dir,'Igrams3'+'_'+str(rlks*additional_rlks)+'_'+str(alks*additional_alks));\n",
    "        os.makedirs(lowres_ifg_dir, exist_ok=True);\n",
    "        with multiprocessing.Pool(processes=12) as pool:\n",
    "            pool.starmap(downsample_int,[(ifg_dir,lowres_ifg_dir,additional_rlks,additional_alks,pair) for pair in pairs_list]);\n",
    "    else:\n",
    "        lowres_ifg_dir= ifg_dir;\n",
    "    for pair in pairs_list:\n",
    "        unwrap_high_low(lowres_ifg_dir,ifg_dir,pair,snaphu_conf,True);\n",
    "        generate_xml_for_unw(ifg_dir,pair,slc_dir,looks);\n",
    "\n",
    "    # with multiprocessing.Pool(processes=12) as pool:\n",
    "    #     pool.starmap(unwrap_high_low,[(ifg_dir,lowres_ifg_dir,pair,snaphu_conf,True) for pair in pairs_list]);\n",
    "    print('Finished Unwrapping');\n",
    "else:\n",
    "    print('Proceed to Unwrap using ISCE unwrap.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab236d",
   "metadata": {},
   "source": [
    "### Unwrap the interferograms using ISCE+SNAPHU\n",
    "- If custom unwrapping is not desired, proceeds with standard unwrapping procedure from ISCE 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unwrap interferograms\n",
    "if unwrapping_protocol == 'isce':\n",
    "    def unwrap_snaphu(unwrap_method,defo_max,looks,slc_dir,ifg_dir,pair):\n",
    "        referenceDate,secondaryDate = pair.split('_');\n",
    "        reference_slc_dir = slc_dir+'/'+referenceDate\n",
    "        int_dir  = os.path.join(ifg_dir,referenceDate+'_'+secondaryDate);\n",
    "        intf =  os.path.join(int_dir,referenceDate+'_'+secondaryDate+'.int');\n",
    "        unwf =  os.path.join(int_dir,referenceDate+'_'+secondaryDate+'.unw');\n",
    "        cohf =  os.path.join(int_dir,referenceDate+'_'+secondaryDate+'.coh');\n",
    "        if not os.path.exists(unwf):\n",
    "            cmd = ['unwrap.py -i {} -c {} -u {} -s {}/ -a {} -r {} -d {} -m {}'.format(intf,cohf,unwf,reference_slc_dir,looks[0],looks[1],defo_max,unwrap_method)];\n",
    "            run_cmd(cmd);\n",
    "        else:\n",
    "            print('Skipping {} as unw file exists'.format(os.path.basename(unwf)));\n",
    "    \n",
    "    unwrap_method = 'snaphu';\n",
    "    defo_max = 2;\n",
    "    for pair in pairs_list:\n",
    "        unwrap_snaphu(unwrap_method, defo_max, looks, slc_dir, ifg_dir, pair);\n",
    "    print('Finished Unwrapping')\n",
    "    \n",
    "#####Currently not working, crashing the instance ########\n",
    "# with multiprocessing.Pool(processes=12) as pool:\n",
    "#     pool.starmap(unwrap_snaphu,[(unwrap_method,defo_max,looks,slc_dir,ifg_dir,pair) for pair in pairs_list]);\n",
    "# if __name__ == \"__main__\":\n",
    "#     with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "#         executor.map(unwrap_snaphu, pairs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a031b8d9",
   "metadata": {},
   "source": [
    "### Generate baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcf225-3113-4640-8429-baa0b21e20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create UAVSAR baselines\n",
    "baselines_dir = os.path.join(out_dir,'baselines');\n",
    "os.makedirs(baselines_dir, exist_ok=True);\n",
    "if len(glob.glob(os.path.join(baselines_dir,'*.txt')))== (len(os.listdir(slc_dir))-1):\n",
    "    print('Baseline directory and files exist. Skipping')\n",
    "\n",
    "else:\n",
    "    cmd ='uavsar_baselines.py -s {} -b {}  -m {}'.format(slc_dir,baselines_dir,reference_slc_dir);\n",
    "    run_cmd(cmd);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb2441",
   "metadata": {},
   "source": [
    "### Generate geometry files from the DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f9e7d-6adf-4623-ae85-65f8b0a0fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate geometry files\n",
    "geometery_dir = os.path.join(out_dir,'geometry');\n",
    "if os.path.exists(os.path.join(geometery_dir,'hgt.rdr')):\n",
    "    print('Geometry files exist. Skipped running topo.py');\n",
    "else:\n",
    "    cmd = 'topo.py -a {} -r {} -d {} -m {} -o {} -n'.format(str(alks), str(rlks),dem_file,reference_slc_dir,geometery_dir);\n",
    "    print(cmd);\n",
    "    run_cmd(cmd);\n",
    "    geometery_dir_mlooked = os.path.join(os.path.dirname(os.path.dirname(geometery_dir)), 'geom_reference');\n",
    "    cmd = 'mv {}/* {}/.'.format(geometery_dir_mlooked,geometery_dir);\n",
    "    print(cmd);\n",
    "    run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a073b4",
   "metadata": {},
   "source": [
    "### Copy referenceShelve directory from the master interferogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c68326b-2f2f-4366-a1ae-ba7ba5748b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create referenceShelve dir\n",
    "reference_dir = os.path.join(out_dir,'referenceShelve');\n",
    "reference_ifg = sorted(glob.glob(ifg_dir+'/'+reference_date+'_*'))[0];\n",
    "print('Reference interferogram is:',reference_ifg)\n",
    "cmd ='cp -r {}/referenceShelve {}'.format(reference_ifg,reference_dir)\n",
    "print(cmd);\n",
    "run_cmd(cmd);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815ef6c",
   "metadata": {},
   "source": [
    "### Prepare interferograms and geometry files to be loaded into MintPy.\n",
    "- This step requires an environment with both ISCE and MintPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2215a6b-8761-41e3-aff4-764319d66196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare .rsc files for unwrapped interferograms to be loaded into MintPy\n",
    "cmd = 'prep_isce.py -f {}/*/{} -m {}/data -b {}  -g {}'.format(ifg_dir,'*.unw',reference_dir,baselines_dir,geometery_dir)\n",
    "print(cmd);\n",
    "run_cmd(cmd);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff898e0c",
   "metadata": {},
   "source": [
    "### Add paths pointing the inteferometric and geometry files to the smallbaselineApp.cfg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae6b6f-68f3-4da4-a51e-0de907a81150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mintpy_dir= os.path.join(out_dir,'mintpy');\n",
    "os.makedirs(mintpy_dir, exist_ok=True);\n",
    "config_file = Path(mintpy_dir)/('smallbaselineApp.cfg')\n",
    "config_file_parameters = \"\"\"\n",
    "####mintpy parameters\n",
    "mintpy.load.processor = isce\n",
    "mintpy.compute.numWorker = auto\n",
    "##path to interferograms\n",
    "mintpy.load.autoPath = no\n",
    "mintpy.load.metaFile       = {out_dir}/referenceShelve/data.dat\n",
    "mintpy.load.baselineDir    = {out_dir}/baselines  \n",
    "mintpy.load.unwFile        = {out_dir}/Igrams/*/*.unw  \n",
    "mintpy.load.corFile        = {out_dir}/Igrams/*/*.coh\n",
    "mintpy.load.connCompFile   = {out_dir}/Igrams/*/*.conncomp  \n",
    "#mintpy.load.intFile        = {out_dir}/Igrams/*/*.int  \n",
    "###path to geometry files\n",
    "mintpy.load.demFile = {geometery_dir}/hgt.rdr\n",
    "mintpy.load.lookupYFile = {geometery_dir}/lat.rdr\n",
    "mintpy.load.lookupXFile = {geometery_dir}/lon.rdr\n",
    "mintpy.load.incAngleFile = {geometery_dir}/los.rdr\n",
    "mintpy.load.azAngleFile = {geometery_dir}/los.rdr\n",
    "mintpy.load.shadowMaskFile = {geometery_dir}/shadowMask.msk\n",
    "\"\"\".format(out_dir=out_dir,geometery_dir=geometery_dir)\n",
    "config_file.write_text(config_file_parameters);\n",
    "print('MintPy config file:\\n    {}:'.format(config_file))\n",
    "print(config_file.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c3971",
   "metadata": {},
   "source": [
    "### Load files to make MintPy input data cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9b2ab-f65e-4dd9-b6e4-584c72e1552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(mintpy_dir);\n",
    "cmd = 'smallbaselineApp.py ' + 'smallbaselineApp.cfg' + ' --dostep load_data'\n",
    "print(cmd);\n",
    "run_cmd(cmd);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be721651",
   "metadata": {},
   "source": [
    "### Process Timeseries inversion and other Mintpy steps (optional)\n",
    "- Proceed only if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437779f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional continue if you like to do mintpy processing here\n",
    "cmd = 'smallbaselineApp.py ' + 'smallbaselineApp.cfg' + ' --start modify_network'\n",
    "print(cmd);\n",
    "run_cmd(cmd);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isce",
   "language": "python",
   "name": "isce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
